---
title: "Machine Learning - Exercise Activity Prediction"
author: "Kent Mok"
date: "November 12, 2015"
output: html_document
---

#Summary
This Exercise Activity Prediction project is a study on human activity 
recognition. The goal of the project is to use data taken during an exercise 
motion to develop a model that can be used to predict how the same exercise is 
done in the future. The particular data for this project is for a Unilateral 
Dumbbell Bicep Curl. The data was gathered from accelerometers on the belt, 
forearm, arm, and dumbell of six participants.

The participants were asked to perform the exercise exactly according to the 
specification (Class A), throwing the elbows to the front (Class B), lifting 
the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class 
D) and throwing the hips to the front (Class E). The data contains an output 
factor variable `classe` to identify these five ways the exercise is done.

More information about the original project can be found 
[here](http://groupware.les.inf.puc-rio.br/har) (Weight Lifting Exercise 
Dataset).

This project studies the model creation and prediction using cross-validation. 
Data from the experiment was provided for model training and cross validation. 
The training data for this project is available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv). 
In addition, test data with 20 observations were provided to evaluate the model.
The results of the predictions for the test data were submitted as output for 
the project. The test data is available 
[here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).

#Load Data and Packages
```{r load_data, message=FALSE}
# Load packages
Installed <- require("caret")
if (!Installed) {
    install.packages("caret")
    library("caret")
}

Installed <- require("randomForest")
if (!Installed) {
    install.packages("randomForest")
    library("randomForest")
}

# Load data
if (!file.exists("pml-training.csv")) {  # Checks if file has been downloaded
    trainurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    download.file(trainurl, "pml-training.csv", method = "curl")
}
if (!file.exists("pml-testing.csv")) {  # Checks if file has been downloaded
    testurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(testurl, "pml-testing.csv", method = "curl")
}
# Read data from files
trainraw <- read.csv("pml-training.csv")
testraw <- read.csv("pml-testing.csv")

# Set seed for repeatability
set.seed(12345)
```

#Clean Data

The original raw data as downloaded contains 160 variables. The variables 
consist of information about the user including identifier and name as well as 
time stamps in addition to the accelerometer measurements and outcome. The 
miscellaneous variables account for seven of the 160 variables.

Of the remaining 153 variables, one is the outcome and a large portion of the 
remaining accelerometer measurement variables had NA values for a majority of 
their observations. These were removed as they do not provide anything for the 
model training.
```{r trim_empty_data}
=======
#Trim Data
The data set consists of some time stamps
```{r trim_data}
# Create empty data frame to store variable names and missing value counts
missingVals <- data.frame(Variable = character(),
                          Missing = integer(),
                          stringsAsFactors = FALSE)
for (i in 1:dim(trainraw)[2]) {
    # Take name of variable
    missingVals[i, 1] <- names(trainraw)[i]
    # Count empty values
    missingVals[i, 2] <- sum(trainraw[, i] == "", na.rm = TRUE)
    # Count NA values
    missingVals[i, 2] <- missingVals[i, 2] + sum(is.na(trainraw[, i]))
}
# Remove variables with missing data
newtrain <- trainraw[, which(missingVals[, 2] == 0)]
dim(newtrain)
```

After removing the variables with empty or mostly empty observations, there were
60 remaining variables. The miscellaneous variables are then removed to cull the
total to 53.

```{r trim_misc_data}
# Remove variables that are not predictors to the outcome
varToRemove <- names(newtrain) %in% c("X", "user_name", "raw_timestamp_part_1",
                                      "raw_timestamp_part_2", "cvtd_timestamp",
                                      "new_window", "num_window")
traindata <- newtrain[!varToRemove]
dim(traindata)

# Correlation could be used to remove even more variables
#corMatrix <- cor(traindata[,1:52])
#highCor <- findCorrelation(corMatrix, cutoff = 0.75)
```

#Predictive Model
In order to perform the cross validation, the training data provided is split
into two smaller data subsets. One is used to train the model, and the other is 
used to test and cross validate the model. The proportion of data used is 70% 
for training and 30% for testing.
```{r cross_validation}
# Split the training set into 70/30 training-validation sets
inTrain <- createDataPartition(traindata$classe, p = 0.7, list = FALSE)
trainSet <- traindata[inTrain,]
testSet <- traindata[-inTrain,]
```
The Random Forest method is used for its accuracy at the cost of efficiency.
```{r predictive_model, cache=TRUE}
modelFitRF <- randomForest(classe ~ ., data = trainSet)
predictionRF <- predict(modelFitRF, testSet)
confMatrix <- confusionMatrix(predictionRF, testSet$classe)
# Accuracy can be found as confMatrix$overall[1]
```

#Out of Sample Error

The expected out-of-sample error is $1-accuracy$ in the cross-validation data. 
Accuracy is the number of correctly classified observations divided by total 
observations in the cross-validation data set (testSet). Therefore, the expected 
out-of-sample error will be 
$1-\frac{Observations_{correct}}{Observations_{total}} = 1 - Accuracy_{testSet}$.

```{r OOS_error}
OOSerror <- sum(predictionRF != testSet$classe) / length(testSet$classe)
```

The out-of-sample error calculated from the model prediction is 
**`r round(OOSerror, 5)`**. The accuracy from the confusion matrix was found as 
**`r round(confMatrix$overall[1], 5)`**. This matches the expected out-of-sample
error.

***

#Project Prediction Submission

The script below runs the prediction with the test data as provided for the
project. There are 20 observations to use for predicting 20 outcomes. The script
creates a unique text file for each of the 20 outcomes.
```{r project_submission}
# Perform prediction
finalPrediction <- predict(modelFitRF, testraw, type = "class")

pml_write_files = function (x) {
    n = length(x)
    for (i in 1:n) {
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i], file = filename, quote = FALSE, row.names = FALSE,
                    col.names = FALSE)
    }
}
pml_write_files(finalPrediction)
```