---
title: "Machine Learning - Exercise Activity Prediction"
author: "Kent Mok"
date: "November 12, 2015"
output: html_document
---

#Summary
This Exercise Activity Prediction project is a study on human activity recognition. The goal of the project is to use data taken during an exercise motion to develop a model that can be used to predict how the same exercise is done in the future. The particular data for this project is for a Unilateral Dumbbell Bicep Curl. The data was gathered from accelerometers on the belt, forearm, arm, and dumbell of six participants.

The participants were asked to perform the exercise exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). The data contains an output factor variable `classe` to identify these five ways the exercise is done.

More information about the original project can be found [here](http://groupware.les.inf.puc-rio.br/har) (Weight Lifting Exercise Dataset).

This project studies the model creation and prediction using cross-validation. Data from the experiment was provided for model training and cross validation. The training data for this project is available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv). In addition, test data with 20 observations were provided to evaluate the model. The results of the predictions for the test data were submitted as output for the project. The test data is available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).

#Load Data and Packages
```{r load_data, message=FALSE}
# Load packages
Installed <- require("caret")
if (!Installed) {
    install.packages("caret")
    library("caret")
}

Installed <- require("randomForest")
if (!Installed) {
    install.packages("randomForest")
    library("randomForest")
}

# Load data
if (!file.exists("pml-training.csv")) {  # Checks if file has been downloaded
    trainurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    download.file(trainurl, "pml-training.csv", method = "curl")
}
if (!file.exists("pml-testing.csv")) {  # Checks if file has been downloaded
    testurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(testurl, "pml-testing.csv", method = "curl")
}
# Read data from files
trainraw <- read.csv("pml-training.csv")
testraw <- read.csv("pml-testing.csv")

# Set seed for repeatability
set.seed(12345)
```

#Trim Data
```{r trim_data}
# Create empty data frame to store variable names and missing value counts
missingVals <- data.frame(Variable = character(),
                          Missing = integer(),
                          stringsAsFactors = FALSE)
for (i in 1:dim(trainraw)[2]) {
    # Take name of variable
    missingVals[i, 1] <- names(trainraw)[i]
    # Count empty values
    missingVals[i, 2] <- sum(trainraw[, i] == "", na.rm = TRUE)
    # Count NA values
    missingVals[i, 2] <- missingVals[i, 2] + sum(is.na(trainraw[, i]))
}
# Remove variables with missing data
newtrain <- trainraw[, which(missingVals[, 2] == 0)]

# Remove variables that are not predictors to the outcome
varToRemove <- names(newtrain) %in% c("X", "user_name", "raw_timestamp_part_1",
                                      "raw_timestamp_part_2", "cvtd_timestamp",
                                      "new_window", "num_window")
traindata <- newtrain[!varToRemove]

corMatrix <- cor(traindata[,1:52])
highCor <- findCorrelation(corMatrix, cutoff = 0.75)
```

#Predictive Model
```{r predictive_model, cache=TRUE}
# Split the training set into 70/30 training-validation sets
inTrain <- createDataPartition(traindata$classe, p = 0.7, list = FALSE)
trainSet <- traindata[inTrain,]
validSet <- traindata[-inTrain,]

modelFitRF <- randomForest(classe ~ ., data = trainSet)
predictionRF <- predict(modelFitRF, validSet)
confMatrix <- confusionMatrix(predictionRF, validSet$classe)
# Accuracy can be found as confMatrix$overall[1]
```

#Out of Sample Error

The expected out-of-sample error is $1-accuracy$ in the cross-validation data. Accuracy is the number of correctly classified observations divided by total observations in the cross-validation data set (validSet). Therefore, the expected out-of-sample error will be $1-\frac{Observations_{correct}}{Observations_{total}} = 1 - Accuracy_{validSet}$.

```{r OOS_error}
OOSerror <- sum(predictionRF != validSet$classe) / length(validSet$classe)
```

The out-of-sample error calculated from the model prediction is 
**`r round(OOSerror, 5)`**. The accuracy from the confusion matrix was found as 
**`r round(confMatrix$overall[1], 5)`**. This matches the expected out-of-sample
error.

***

#Project Prediction Submission

The script below runs the prediction with the test data as provided for the
project. There are 20 observations to use for predicting 20 outcomes. The script
creates a unique text file for each of the 20 outcomes.
```{r project_submission}
# Perform prediction
finalPrediction <- predict(modelFitRF, testraw, type = "class")

pml_write_files = function (x) {
    n = length(x)
    for (i in 1:n) {
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i], file = filename, quote = FALSE, row.names = FALSE,
                    col.names = FALSE)
    }
}
pml_write_files(finalPrediction)
```